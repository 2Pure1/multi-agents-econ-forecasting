{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOIub5B-S83Z"
      },
      "source": [
        "# Agent Evaluation and Testing\n",
        "## Performance Assessment and Quality Assurance - Notebook 6\n",
        "\n",
        "**Objective**: Systematically evaluate and test the performance of all agents in the economic forecasting multi-agent system.\n",
        "\n",
        "### What You'll Learn:\n",
        "- Agent performance metrics and evaluation\n",
        "- Forecast accuracy testing and validation\n",
        "- Tool usage efficiency analysis\n",
        "- Session management evaluation\n",
        "- Performance benchmarking and optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GqL-2ZDS83d"
      },
      "source": [
        "## 1. Setup and Evaluation Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmnIKp4qS83d"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import asyncio\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import json\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "\n",
        "# Import evaluation components\n",
        "from agents.team_coordinator import EconomicTeamCoordinator\n",
        "from tools.statistical_tools import StatisticalTools\n",
        "from google.adk.models.google_llm import Gemini\n",
        "from google.genai import types\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "\n",
        "# Setup visualization\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Evaluation framework components imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQHX4CSkS83f"
      },
      "outputs": [],
      "source": [
        "# Initialize evaluation system\n",
        "print(\"üöÄ Initializing Agent Evaluation System...\")\n",
        "\n",
        "# Initialize components\n",
        "stat_tools = StatisticalTools()\n",
        "\n",
        "# Initialize model\n",
        "retry_config = types.HttpRetryOptions(\n",
        "    attempts=5,\n",
        "    exp_base=7,\n",
        "    initial_delay=1,\n",
        "    http_status_codes=[429, 500, 503, 504],\n",
        ")\n",
        "\n",
        "model = Gemini(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    retry_options=retry_config\n",
        ")\n",
        "\n",
        "# Initialize team coordinator\n",
        "bea_api_key = os.getenv('BEA_API_KEY')\n",
        "team_coordinator = EconomicTeamCoordinator(bea_api_key, model)\n",
        "\n",
        "print(\"ü§ñ Evaluation system initialized with all agents\")\n",
        "print(\"üìä Ready to perform comprehensive agent evaluation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVCC5e2hS83f"
      },
      "source": [
        "## 2. Test Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF6BIrRfS83g"
      },
      "outputs": [],
      "source": [
        "# Generate comprehensive test datasets\n",
        "def generate_test_datasets():\n",
        "    \"\"\"Generate multiple test datasets for agent evaluation\"\"\"\n",
        "\n",
        "    # Base dates\n",
        "    dates = pd.date_range(start='2010-01-01', end='2024-12-31', freq='Q')\n",
        "\n",
        "    test_datasets = {}\n",
        "\n",
        "    # Dataset 1: Stable growth scenario\n",
        "    np.random.seed(42)\n",
        "    stable_trend = np.linspace(100, 200, len(dates))\n",
        "    stable_seasonal = 5 * np.sin(2 * np.pi * np.arange(len(dates)) / 4)\n",
        "    stable_noise = np.random.normal(0, 2, len(dates))\n",
        "    test_datasets['stable_growth'] = pd.DataFrame({\n",
        "        'TimePeriod': dates,\n",
        "        'DataValue': stable_trend + stable_seasonal + stable_noise,\n",
        "        'Scenario': 'Stable Growth'\n",
        "    })\n",
        "\n",
        "    # Dataset 2: Volatile scenario\n",
        "    np.random.seed(43)\n",
        "    volatile_trend = np.linspace(100, 180, len(dates))\n",
        "    volatile_cycle = 20 * np.sin(2 * np.pi * np.arange(len(dates)) / 8)\n",
        "    volatile_noise = np.random.normal(0, 8, len(dates))\n",
        "    test_datasets['volatile'] = pd.DataFrame({\n",
        "        'TimePeriod': dates,\n",
        "        'DataValue': volatile_trend + volatile_cycle + volatile_noise,\n",
        "        'Scenario': 'High Volatility'\n",
        "    })\n",
        "\n",
        "    # Dataset 3: Recession scenario\n",
        "    np.random.seed(44)\n",
        "    recession_trend = np.linspace(150, 250, len(dates))\n",
        "    # Add recession around midpoint\n",
        "    recession_depth = 30\n",
        "    recession_start = len(dates) // 3\n",
        "    recession_end = recession_start + 8\n",
        "    recession_effect = np.zeros(len(dates))\n",
        "    for i in range(len(dates)):\n",
        "        if recession_start <= i < recession_end:\n",
        "            recession_effect[i] = -recession_depth * (1 - (i - recession_start) / (recession_end - recession_start))\n",
        "    recession_noise = np.random.normal(0, 3, len(dates))\n",
        "    test_datasets['recession'] = pd.DataFrame({\n",
        "        'TimePeriod': dates,\n",
        "        'DataValue': recession_trend + recession_effect + recession_noise,\n",
        "        'Scenario': 'Recession Scenario'\n",
        "    })\n",
        "\n",
        "    # Dataset 4: Structural break scenario\n",
        "    np.random.seed(45)\n",
        "    break_trend1 = np.linspace(100, 150, len(dates)//2)\n",
        "    break_trend2 = np.linspace(120, 220, len(dates) - len(dates)//2)\n",
        "    structural_trend = np.concatenate([break_trend1, break_trend2])\n",
        "    structural_noise = np.random.normal(0, 4, len(dates))\n",
        "    test_datasets['structural_break'] = pd.DataFrame({\n",
        "        'TimePeriod': dates,\n",
        "        'DataValue': structural_trend + structural_noise,\n",
        "        'Scenario': 'Structural Break'\n",
        "    })\n",
        "\n",
        "    return test_datasets\n",
        "\n",
        "# Generate test datasets\n",
        "test_datasets = generate_test_datasets()\n",
        "\n",
        "print(\"üìä Test Datasets Generated:\")\n",
        "for scenario, data in test_datasets.items():\n",
        "    print(f\"   ‚Ä¢ {scenario}: {len(data)} quarters, {data['Scenario'].iloc[0]}\")\n",
        "\n",
        "# Display sample of each dataset\n",
        "print(\"\\nüîç Sample Data from Each Scenario:\")\n",
        "for scenario, data in test_datasets.items():\n",
        "    print(f\"\\n{scenario}:\")\n",
        "    print(data[['TimePeriod', 'DataValue']].head(3).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMUPxbPLS83g"
      },
      "source": [
        "## 3. Data Collector Agent Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LR9sw5wS83h"
      },
      "outputs": [],
      "source": [
        "# Evaluate Data Collector Agent\n",
        "print(\"üì• Evaluating Data Collector Agent...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "async def evaluate_data_collector():\n",
        "    \"\"\"Comprehensive evaluation of Data Collector Agent\"\"\"\n",
        "\n",
        "    evaluation_results = {}\n",
        "\n",
        "    # Test 1: GDP Data Collection\n",
        "    print(\"\\nüß™ Test 1: GDP Data Collection\")\n",
        "    gdp_result = await team_coordinator.data_collector.get_gdp_data()\n",
        "\n",
        "    evaluation_results['gdp_collection'] = {\n",
        "        'status': gdp_result['status'],\n",
        "        'data_points': len(gdp_result.get('data', [])),\n",
        "        'success': gdp_result['status'] == 'success',\n",
        "        'message': gdp_result.get('message', 'No message')\n",
        "    }\n",
        "\n",
        "    print(f\"   Status: {gdp_result['status']}\")\n",
        "    print(f\"   Data Points: {len(gdp_result.get('data', []))}\")\n",
        "    print(f\"   Message: {gdp_result.get('message', 'No message')}\")\n",
        "\n",
        "    # Test 2: Unemployment Data Collection\n",
        "    print(\"\\nüß™ Test 2: Unemployment Data Collection\")\n",
        "    unemployment_result = await team_coordinator.data_collector.get_unemployment_data()\n",
        "\n",
        "    evaluation_results['unemployment_collection'] = {\n",
        "        'status': unemployment_result['status'],\n",
        "        'data_points': len(unemployment_result.get('data', [])),\n",
        "        'success': unemployment_result['status'] == 'success',\n",
        "        'message': unemployment_result.get('message', 'No message')\n",
        "    }\n",
        "\n",
        "    print(f\"   Status: {unemployment_result['status']}\")\n",
        "    print(f\"   Data Points: {len(unemployment_result.get('data', []))}\")\n",
        "    print(f\"   Message: {unemployment_result.get('message', 'No message')}\")\n",
        "\n",
        "    # Test 3: Inflation Data Collection\n",
        "    print(\"\\nüß™ Test 3: Inflation Data Collection\")\n",
        "    inflation_result = await team_coordinator.data_collector.get_inflation_data()\n",
        "\n",
        "    evaluation_results['inflation_collection'] = {\n",
        "        'status': inflation_result['status'],\n",
        "        'data_points': len(inflation_result.get('data', [])),\n",
        "        'success': inflation_result['status'] == 'success',\n",
        "        'message': inflation_result.get('message', 'No message')\n",
        "    }\n",
        "\n",
        "    print(f\"   Status: {inflation_result['status']}\")\n",
        "    print(f\"   Data Points: {len(inflation_result.get('data', []))}\")\n",
        "    print(f\"   Message: {inflation_result.get('message', 'No message')}\")\n",
        "\n",
        "    # Calculate overall performance\n",
        "    successful_tests = sum(1 for test in evaluation_results.values() if test['success'])\n",
        "    total_tests = len(evaluation_results)\n",
        "    success_rate = (successful_tests / total_tests) * 100\n",
        "\n",
        "    evaluation_results['summary'] = {\n",
        "        'successful_tests': successful_tests,\n",
        "        'total_tests': total_tests,\n",
        "        'success_rate': success_rate,\n",
        "        'agent': 'Data Collector'\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä Data Collector Agent Summary:\")\n",
        "    print(f\"   Successful Tests: {successful_tests}/{total_tests}\")\n",
        "    print(f\"   Success Rate: {success_rate:.1f}%\")\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "# Run data collector evaluation\n",
        "data_collector_results = await evaluate_data_collector()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVEMY3z_S83h"
      },
      "source": [
        "## 4. Economic Analyst Agent Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87eXFSwBS83h"
      },
      "outputs": [],
      "source": [
        "# Evaluate Economic Analyst Agent\n",
        "print(\"üìä Evaluating Economic Analyst Agent...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "async def evaluate_economic_analyst():\n",
        "    \"\"\"Comprehensive evaluation of Economic Analyst Agent\"\"\"\n",
        "\n",
        "    evaluation_results = {}\n",
        "\n",
        "    # Test each scenario\n",
        "    for scenario_name, test_data in test_datasets.items():\n",
        "        print(f\"\\nüß™ Testing Scenario: {scenario_name}\")\n",
        "\n",
        "        data_dict = test_data[['TimePeriod', 'DataValue']].to_dict('records')\n",
        "\n",
        "        # Test 1: Growth Trend Analysis\n",
        "        growth_result = await team_coordinator.economic_analyst.analyze_growth_trends(data_dict)\n",
        "\n",
        "        # Test 2: Economic Indicators\n",
        "        indicator_result = await team_coordinator.economic_analyst.calculate_economic_indicators(data_dict)\n",
        "\n",
        "        # Test 3: Business Cycle Analysis\n",
        "        cycle_result = await team_coordinator.economic_analyst.identify_business_cycles(data_dict)\n",
        "\n",
        "        # Test 4: Anomaly Detection\n",
        "        anomaly_result = await team_coordinator.economic_analyst.detect_anomalies(data_dict)\n",
        "\n",
        "        # Store results\n",
        "        evaluation_results[scenario_name] = {\n",
        "            'growth_analysis': growth_result['status'] == 'success',\n",
        "            'indicators': indicator_result['status'] == 'success',\n",
        "            'business_cycles': cycle_result['status'] == 'success',\n",
        "            'anomaly_detection': anomaly_result['status'] == 'success'\n",
        "        }\n",
        "\n",
        "        # Print scenario results\n",
        "        successful_tests = sum(evaluation_results[scenario_name].values())\n",
        "        total_tests = len(evaluation_results[scenario_name])\n",
        "\n",
        "        print(f\"   Successful: {successful_tests}/{total_tests} tests\")\n",
        "        print(f\"   Growth Analysis: {'‚úÖ' if growth_result['status'] == 'success' else '‚ùå'}\")\n",
        "        print(f\"   Indicators: {'‚úÖ' if indicator_result['status'] == 'success' else '‚ùå'}\")\n",
        "        print(f\"   Business Cycles: {'‚úÖ' if cycle_result['status'] == 'success' else '‚ùå'}\")\n",
        "        print(f\"   Anomaly Detection: {'‚úÖ' if anomaly_result['status'] == 'success' else '‚ùå'}\")\n",
        "\n",
        "    # Calculate overall performance\n",
        "    all_tests = []\n",
        "    for scenario_results in evaluation_results.values():\n",
        "        all_tests.extend(scenario_results.values())\n",
        "\n",
        "    successful_total = sum(all_tests)\n",
        "    total_tests = len(all_tests)\n",
        "    success_rate = (successful_total / total_tests) * 100\n",
        "\n",
        "    evaluation_results['summary'] = {\n",
        "        'successful_tests': successful_total,\n",
        "        'total_tests': total_tests,\n",
        "        'success_rate': success_rate,\n",
        "        'agent': 'Economic Analyst'\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä Economic Analyst Agent Summary:\")\n",
        "    print(f\"   Successful Tests: {successful_total}/{total_tests}\")\n",
        "    print(f\"   Success Rate: {success_rate:.1f}%\")\n",
        "    print(f\"   Scenarios Tested: {len(test_datasets)}\")\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "# Run economic analyst evaluation\n",
        "economic_analyst_results = await evaluate_economic_analyst()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF_FC01PS83i"
      },
      "source": [
        "## 5. Forecasting Specialist Agent Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyns2klAS83j"
      },
      "outputs": [],
      "source": [
        "# Evaluate Forecasting Specialist Agent\n",
        "print(\"üîÆ Evaluating Forecasting Specialist Agent...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "async def evaluate_forecasting_specialist():\n",
        "    \"\"\"Comprehensive evaluation of Forecasting Specialist Agent\"\"\"\n",
        "\n",
        "    evaluation_results = {}\n",
        "    accuracy_metrics = {}\n",
        "\n",
        "    for scenario_name, test_data in test_datasets.items():\n",
        "        print(f\"\\nüß™ Testing Scenario: {scenario_name}\")\n",
        "\n",
        "        # Split data for train/test\n",
        "        split_point = int(len(test_data) * 0.7)\n",
        "        train_data = test_data.iloc[:split_point]\n",
        "        test_data_actual = test_data.iloc[split_point:]\n",
        "\n",
        "        train_dict = train_data[['TimePeriod', 'DataValue']].to_dict('records')\n",
        "\n",
        "        # Test 1: GDP Forecasting\n",
        "        forecast_result = await team_coordinator.forecasting_specialist.forecast_gdp(\n",
        "            train_dict,\n",
        "            horizon=len(test_data_actual)\n",
        "        )\n",
        "\n",
        "        # Test 2: ARIMA Model Building\n",
        "        arima_result = await team_coordinator.forecasting_specialist.build_arima_model(train_dict)\n",
        "\n",
        "        # Test 3: Ensemble Forecasting\n",
        "        ensemble_result = await team_coordinator.forecasting_specialist.generate_ensemble_forecast(train_dict)\n",
        "\n",
        "        # Calculate accuracy if forecasts were successful\n",
        "        if forecast_result['status'] == 'success':\n",
        "            actual_values = test_data_actual['DataValue'].values\n",
        "            predicted_values = [f['point_forecast'] for f in forecast_result.get('forecasts', [])]\n",
        "\n",
        "            if len(predicted_values) == len(actual_values):\n",
        "                mae = mean_absolute_error(actual_values, predicted_values)\n",
        "                rmse = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
        "                mape = np.mean(np.abs((actual_values - predicted_values) / actual_values)) * 100\n",
        "\n",
        "                accuracy_metrics[scenario_name] = {\n",
        "                    'mae': mae,\n",
        "                    'rmse': rmse,\n",
        "                    'mape': mape\n",
        "                }\n",
        "\n",
        "        # Store results\n",
        "        evaluation_results[scenario_name] = {\n",
        "            'gdp_forecasting': forecast_result['status'] == 'success',\n",
        "            'arima_modeling': arima_result['status'] == 'success',\n",
        "            'ensemble_forecasting': ensemble_result['status'] == 'success',\n",
        "            'accuracy_available': forecast_result['status'] == 'success' and len(predicted_values) == len(actual_values)\n",
        "        }\n",
        "\n",
        "        # Print scenario results\n",
        "        successful_tests = sum(evaluation_results[scenario_name].values())\n",
        "        total_tests = len(evaluation_results[scenario_name])\n",
        "\n",
        "        print(f\"   Successful: {successful_tests}/{total_tests} tests\")\n",
        "        print(f\"   GDP Forecasting: {'‚úÖ' if forecast_result['status'] == 'success' else '‚ùå'}\")\n",
        "        print(f\"   ARIMA Modeling: {'‚úÖ' if arima_result['status'] == 'success' else '‚ùå'}\")\n",
        "        print(f\"   Ensemble Forecasting: {'‚úÖ' if ensemble_result['status'] == 'success' else '‚ùå'}\")\n",
        "\n",
        "        if scenario_name in accuracy_metrics:\n",
        "            print(f\"   Forecast Accuracy: MAE={accuracy_metrics[scenario_name]['mae']:.2f}, \"\n",
        "                  f\"RMSE={accuracy_metrics[scenario_name]['rmse']:.2f}\")\n",
        "\n",
        "    # Calculate overall performance\n",
        "    all_tests = []\n",
        "    for scenario_results in evaluation_results.values():\n",
        "        all_tests.extend(scenario_results.values())\n",
        "\n",
        "    successful_total = sum(all_tests)\n",
        "    total_tests = len(all_tests)\n",
        "    success_rate = (successful_total / total_tests) * 100\n",
        "\n",
        "    evaluation_results['summary'] = {\n",
        "        'successful_tests': successful_total,\n",
        "        'total_tests': total_tests,\n",
        "        'success_rate': success_rate,\n",
        "        'agent': 'Forecasting Specialist',\n",
        "        'accuracy_metrics': accuracy_metrics\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä Forecasting Specialist Agent Summary:\")\n",
        "    print(f\"   Successful Tests: {successful_total}/{total_tests}\")\n",
        "    print(f\"   Success Rate: {success_rate:.1f}%\")\n",
        "    print(f\"   Scenarios Tested: {len(test_datasets)}\")\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "# Run forecasting specialist evaluation\n",
        "forecasting_results = await evaluate_forecasting_specialist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEwDwdzBS83k"
      },
      "source": [
        "## 6. Visualization Agent Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpFsYV-VS83k"
      },
      "outputs": [],
      "source": [
        "# Evaluate Visualization Agent\n",
        "print(\"üìà Evaluating Visualization Agent...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "async def evaluate_visualization_agent():\n",
        "    \"\"\"Comprehensive evaluation of Visualization Agent\"\"\"\n",
        "\n",
        "    evaluation_results = {}\n",
        "\n",
        "    # Use stable growth scenario for visualization tests\n",
        "    test_data = test_datasets['stable_growth']\n",
        "    data_dict = test_data[['TimePeriod', 'DataValue']].to_dict('records')\n",
        "\n",
        "    print(\"üß™ Running Visualization Tests...\")\n",
        "\n",
        "    # Test 1: Growth Chart Creation\n",
        "    print(\"\\nüî∏ Test 1: Growth Chart Creation\")\n",
        "    growth_chart_result = await team_coordinator.visualization_agent.create_growth_chart(data_dict)\n",
        "    evaluation_results['growth_chart'] = growth_chart_result['status'] == 'success'\n",
        "    print(f\"   Status: {'‚úÖ Success' if growth_chart_result['status'] == 'success' else '‚ùå Failed'}\")\n",
        "\n",
        "    # Test 2: Forecast Visualization\n",
        "    print(\"\\nüî∏ Test 2: Forecast Visualization\")\n",
        "\n",
        "    # Create sample forecast data\n",
        "    sample_forecast = {\n",
        "        'forecasts': [\n",
        "            {'point_forecast': 210, 'confidence_lower': 205, 'confidence_upper': 215},\n",
        "            {'point_forecast': 212, 'confidence_lower': 207, 'confidence_upper': 217},\n",
        "            {'point_forecast': 215, 'confidence_lower': 210, 'confidence_upper': 220}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    forecast_viz_result = await team_coordinator.visualization_agent.plot_forecasts(data_dict, sample_forecast)\n",
        "    evaluation_results['forecast_visualization'] = forecast_viz_result['status'] == 'success'\n",
        "    print(f\"   Status: {'‚úÖ Success' if forecast_viz_result['status'] == 'success' else '‚ùå Failed'}\")\n",
        "\n",
        "    # Test 3: Comprehensive Dashboard\n",
        "    print(\"\\nüî∏ Test 3: Comprehensive Dashboard\")\n",
        "\n",
        "    sample_analysis = {\n",
        "        'trend': 'upward',\n",
        "        'confidence': 0.85,\n",
        "        'current_growth_rate': 2.5\n",
        "    }\n",
        "\n",
        "    dashboard_result = await team_coordinator.visualization_agent.create_economic_dashboard(\n",
        "        data_dict, sample_analysis, sample_forecast\n",
        "    )\n",
        "    evaluation_results['dashboard_creation'] = dashboard_result['status'] == 'success'\n",
        "    print(f\"   Status: {'‚úÖ Success' if dashboard_result['status'] == 'success' else '‚ùå Failed'}\")\n",
        "\n",
        "    # Test 4: Export Functionality\n",
        "    print(\"\\nüî∏ Test 4: Export Functionality\")\n",
        "    export_result = await team_coordinator.visualization_agent.export_visualization(\"test_dashboard\", \"html\")\n",
        "    evaluation_results['export_functionality'] = export_result['status'] == 'success'\n",
        "    print(f\"   Status: {'‚úÖ Success' if export_result['status'] == 'success' else '‚ùå Failed'}\")\n",
        "\n",
        "    # Calculate overall performance\n",
        "    successful_tests = sum(evaluation_results.values())\n",
        "    total_tests = len(evaluation_results)\n",
        "    success_rate = (successful_tests / total_tests) * 100\n",
        "\n",
        "    evaluation_results['summary'] = {\n",
        "        'successful_tests': successful_tests,\n",
        "        'total_tests': total_tests,\n",
        "        'success_rate': success_rate,\n",
        "        'agent': 'Visualization Agent'\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìä Visualization Agent Summary:\")\n",
        "    print(f\"   Successful Tests: {successful_tests}/{total_tests}\")\n",
        "    print(f\"   Success Rate: {success_rate:.1f}%\")\n",
        "\n",
        "    return evaluation_results\n",
        "\n",
        "# Run visualization agent evaluation\n",
        "visualization_results = await evaluate_visualization_agent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHlizQwTS83k"
      },
      "source": [
        "## 7. Comprehensive Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5QAV0sKS83l"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive performance dashboard\n",
        "print(\"üìä Creating Comprehensive Performance Dashboard...\")\n",
        "\n",
        "# Collect all evaluation results\n",
        "all_results = {\n",
        "    'Data Collector': data_collector_results['summary'],\n",
        "    'Economic Analyst': economic_analyst_results['summary'],\n",
        "    'Forecasting Specialist': forecasting_results['summary'],\n",
        "    'Visualization Agent': visualization_results['summary']\n",
        "}\n",
        "\n",
        "# Create performance comparison\n",
        "performance_data = []\n",
        "for agent, results in all_results.items():\n",
        "    performance_data.append({\n",
        "        'Agent': agent,\n",
        "        'Success Rate (%)': results['success_rate'],\n",
        "        'Successful Tests': results['successful_tests'],\n",
        "        'Total Tests': results['total_tests']\n",
        "    })\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "\n",
        "print(\"\\nüèÜ AGENT PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "# Create performance visualization\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        'Agent Success Rates',\n",
        "        'Test Completion Overview',\n",
        "        'Forecasting Accuracy by Scenario',\n",
        "        'Performance Distribution'\n",
        "    ),\n",
        "    specs=[[{'type': 'bar'}, {'type': 'bar'}], [{'type': 'bar'}, {'type': 'box'}]]\n",
        ")\n",
        "\n",
        "# Plot 1: Success Rates\n",
        "fig.add_trace(\n",
        "    go.Bar(x=performance_df['Agent'], y=performance_df['Success Rate (%)'],\n",
        "           name='Success Rate', marker_color='#1f77b4'),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Plot 2: Test Completion\n",
        "fig.add_trace(\n",
        "    go.Bar(x=performance_df['Agent'], y=performance_df['Successful Tests'],\n",
        "           name='Successful Tests', marker_color='#2ca02c'),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Plot 3: Forecasting Accuracy (if available)\n",
        "if 'accuracy_metrics' in forecasting_results['summary']:\n",
        "    accuracy_data = forecasting_results['summary']['accuracy_metrics']\n",
        "    scenarios = list(accuracy_data.keys())\n",
        "    mae_values = [accuracy_data[scenario]['mae'] for scenario in scenarios]\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Bar(x=scenarios, y=mae_values, name='MAE by Scenario', marker_color='#ff7f0e'),\n",
        "        row=2, col=1\n",
        "    )\n",
        "\n",
        "# Plot 4: Performance Distribution\n",
        "success_rates = performance_df['Success Rate (%)'].tolist()\n",
        "fig.add_trace(\n",
        "    go.Box(y=success_rates, name='Success Rate Distribution', marker_color='#9467bd'),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_text=\"Multi-Agent System Performance Dashboard\",\n",
        "    height=600,\n",
        "    showlegend=False\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJsoi7RNS83m"
      },
      "source": [
        "## 8. Summary and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmb0AMOgS83n"
      },
      "outputs": [],
      "source": [
        "print(\"üéØ EVALUATION SUMMARY AND RECOMMENDATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate overall system performance\n",
        "total_successful = sum(result['summary']['successful_tests'] for result in [\n",
        "    data_collector_results, economic_analyst_results, forecasting_results, visualization_results\n",
        "])\n",
        "total_tests = sum(result['summary']['total_tests'] for result in [\n",
        "    data_collector_results, economic_analyst_results, forecasting_results, visualization_results\n",
        "])\n",
        "overall_success_rate = (total_successful / total_tests) * 100\n",
        "\n",
        "print(f\"üìä OVERALL SYSTEM PERFORMANCE:\")\n",
        "print(f\"   Total Tests: {total_tests}\")\n",
        "print(f\"   Successful Tests: {total_successful}\")\n",
        "print(f\"   Overall Success Rate: {overall_success_rate:.1f}%\")\n",
        "\n",
        "print(f\"\\nüèÜ AGENT PERFORMANCE RANKING:\")\n",
        "ranked_agents = sorted(performance_df.to_dict('records'), key=lambda x: x['Success Rate (%)'], reverse=True)\n",
        "for i, agent in enumerate(ranked_agents, 1):\n",
        "    print(f\"   {i}. {agent['Agent']}: {agent['Success Rate (%)']:.1f}%\")\n",
        "\n",
        "print(f\"\\nüîç KEY FINDINGS:\")\n",
        "\n",
        "# Data Collector Findings\n",
        "dc_success = data_collector_results['summary']['success_rate']\n",
        "if dc_success < 80:\n",
        "    print(f\"   ‚Ä¢ Data Collector: API integration needs improvement ({dc_success:.1f}%)\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Data Collector: Strong performance ({dc_success:.1f}%)\")\n",
        "\n",
        "# Economic Analyst Findings\n",
        "ea_success = economic_analyst_results['summary']['success_rate']\n",
        "if ea_success < 85:\n",
        "    print(f\"   ‚Ä¢ Economic Analyst: Analysis consistency needs work ({ea_success:.1f}%)\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Economic Analyst: Reliable analysis ({ea_success:.1f}%)\")\n",
        "\n",
        "# Forecasting Specialist Findings\n",
        "fs_success = forecasting_results['summary']['success_rate']\n",
        "if 'accuracy_metrics' in forecasting_results['summary']:\n",
        "    avg_mae = np.mean([metrics['mae'] for metrics in forecasting_results['summary']['accuracy_metrics'].values()])\n",
        "    print(f\"   ‚Ä¢ Forecasting Specialist: Good success ({fs_success:.1f}%), Average MAE: {avg_mae:.2f}\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Forecasting Specialist: Moderate success ({fs_success:.1f}%)\")\n",
        "\n",
        "# Visualization Agent Findings\n",
        "va_success = visualization_results['summary']['success_rate']\n",
        "if va_success < 90:\n",
        "    print(f\"   ‚Ä¢ Visualization Agent: Dashboard creation needs optimization ({va_success:.1f}%)\")\n",
        "else:\n",
        "    print(f\"   ‚Ä¢ Visualization Agent: Excellent performance ({va_success:.1f}%)\")\n",
        "\n",
        "print(f\"\\nüí° RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
        "\n",
        "if overall_success_rate < 80:\n",
        "    print(\"   1. Focus on agent reliability and error handling\")\n",
        "    print(\"   2. Implement better retry mechanisms for API calls\")\n",
        "    print(\"   3. Add comprehensive logging and monitoring\")\n",
        "else:\n",
        "    print(\"   1. Maintain current reliability standards\")\n",
        "    print(\"   2. Focus on performance optimization\")\n",
        "    print(\"   3. Expand test coverage for edge cases\")\n",
        "\n",
        "print(f\"\\nüîß TECHNICAL RECOMMENDATIONS:\")\n",
        "print(\"   1. Implement automated testing pipeline\")\n",
        "print(\"   2. Add performance benchmarking\")\n",
        "print(\"   3. Create alerting for performance degradation\")\n",
        "print(\"   4. Establish regular evaluation schedule\")\n",
        "\n",
        "print(f\"\\nüöÄ PRODUCTION READINESS ASSESSMENT:\")\n",
        "if overall_success_rate >= 90:\n",
        "    print(\"   ‚úÖ EXCELLENT - Ready for production deployment\")\n",
        "elif overall_success_rate >= 80:\n",
        "    print(\"   ‚úÖ GOOD - Suitable for production with monitoring\")\n",
        "elif overall_success_rate >= 70:\n",
        "    print(\"   ‚ö†Ô∏è  FAIR - Needs improvement before production\")\n",
        "else:\n",
        "    print(\"   ‚ùå POOR - Significant work needed before production\")\n",
        "\n",
        "print(f\"\\nüìà NEXT STEPS FOR EVALUATION:\")\n",
        "print(\"   1. Implement continuous integration testing\")\n",
        "print(\"   2. Set up performance monitoring dashboard\")\n",
        "print(\"   3. Establish quality gates for deployment\")\n",
        "print(\"   4. Create user acceptance testing framework\")\n",
        "\n",
        "print(f\"\\nüéØ EVALUATION COMPLETE!\")\n",
        "print(\"   The multi-agent system has been thoroughly evaluated and is ready for the next phase.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}